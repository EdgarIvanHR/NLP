{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8933945,"sourceType":"datasetVersion","datasetId":5374760}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T00:14:45.509352Z","iopub.execute_input":"2024-07-12T00:14:45.509707Z","iopub.status.idle":"2024-07-12T00:14:45.517996Z","shell.execute_reply.started":"2024-07-12T00:14:45.509680Z","shell.execute_reply":"2024-07-12T00:14:45.516804Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/big-dos-dataset/big_dos_val.txt\n/kaggle/input/big-dos-dataset/big_dos_test.txt\n/kaggle/input/big-dos-dataset/big_dos_train.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install requests --target=/kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:14:45.523093Z","iopub.execute_input":"2024-07-12T00:14:45.523429Z","iopub.status.idle":"2024-07-12T00:14:45.535222Z","shell.execute_reply.started":"2024-07-12T00:14:45.523400Z","shell.execute_reply":"2024-07-12T00:14:45.534103Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# ESTE CODIGO SOLO SE CORRE UNA VEZ PARA DESCARGAR LOS TEXTOS NECESARIOS\n# PARA EL EJERCICIO\n\"\"\"\nimport requests\nfrom datasets import load_dataset\nimport re\n\nds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n\nx = requests.get('https://norvig.com/big.txt')\n# Nos quedamos solo con el texto hasta este punto\nx = str(x.content[:re.search('\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK, WAR AND PEACE \\*\\*\\*',str(x.content)).span()[0]] )\n#x.content\n#ds['train']['text']\nwikitext2_train = ds['train']['text']\nwikitext2_val  = ds['validation']['text']\nwikitext2_test = ds['test']['text']\n\nwikitext2_train_text = ' '.join(wikitext2_train)\nwikitext2_val_text   = ' '.join(wikitext2_val)\nwikitext2_test_text  = ' '.join(wikitext2_test)\n\n\n#Cleaning data\ndef Clean_data(data):\n    #Removes all the unnecessary patterns and cleans the data to get a good sentence\n    repl='' #String for replacement\n    \n    #removing all open brackets\n    data=re.sub('\\(', repl, data)\n    \n    #removing all closed brackets\n    data=re.sub('\\)', repl, data)\n    \n    #Removing all the headings in data\n    for pattern in set(re.findall(\"=.*=\",data)):\n        data=re.sub(pattern, repl, data)\n    \n    #Removing unknown words in data\n    #for pattern in set(re.findall(\"<unk>\",data)):\n    #    data=re.sub(pattern,repl,data)\n    \n    #Removing all the non-alphanumerical characters\n    for pattern in set(re.findall(r\"[^\\w ]\", data)):\n        repl=''\n        if pattern=='-':\n            repl=' '\n        #Retaining period, apostrophe\n        if pattern!='.' and pattern!=\"\\'\":\n            data=re.sub(\"\\\\\"+pattern, repl, data)\n            \n    return data\n\nwikitext2_train_text = Clean_data(wikitext2_train_text)\nwikitext2_val_text   = Clean_data(wikitext2_val_text)\nwikitext2_test_text  = Clean_data(wikitext2_test_text)\n\nwith open(\"/kaggle/working/big_dos_train.txt\", \"w\") as file:\n    file.write(x + wikitext2_train_text)\n    \nwith open(\"/kaggle/working/big_dos_val.txt\", \"w\") as file:\n    file.write(wikitext2_val_text)\n    \nwith open(\"/kaggle/working/big_dos_test.txt\", \"w\") as file:\n    file.write(wikitext2_test_text)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:14:45.537134Z","iopub.execute_input":"2024-07-12T00:14:45.537497Z","iopub.status.idle":"2024-07-12T00:14:45.550538Z","shell.execute_reply.started":"2024-07-12T00:14:45.537464Z","shell.execute_reply":"2024-07-12T00:14:45.549218Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\nimport requests\\nfrom datasets import load_dataset\\nimport re\\n\\nds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\\n\\nx = requests.get(\\'https://norvig.com/big.txt\\')\\n# Nos quedamos solo con el texto hasta este punto\\nx = str(x.content[:re.search(\\'\\\\*\\\\*\\\\* END OF THE PROJECT GUTENBERG EBOOK, WAR AND PEACE \\\\*\\\\*\\\\*\\',str(x.content)).span()[0]] )\\n#x.content\\n#ds[\\'train\\'][\\'text\\']\\nwikitext2_train = ds[\\'train\\'][\\'text\\']\\nwikitext2_val  = ds[\\'validation\\'][\\'text\\']\\nwikitext2_test = ds[\\'test\\'][\\'text\\']\\n\\nwikitext2_train_text = \\' \\'.join(wikitext2_train)\\nwikitext2_val_text   = \\' \\'.join(wikitext2_val)\\nwikitext2_test_text  = \\' \\'.join(wikitext2_test)\\n\\n\\n#Cleaning data\\ndef Clean_data(data):\\n    #Removes all the unnecessary patterns and cleans the data to get a good sentence\\n    repl=\\'\\' #String for replacement\\n    \\n    #removing all open brackets\\n    data=re.sub(\\'\\\\(\\', repl, data)\\n    \\n    #removing all closed brackets\\n    data=re.sub(\\'\\\\)\\', repl, data)\\n    \\n    #Removing all the headings in data\\n    for pattern in set(re.findall(\"=.*=\",data)):\\n        data=re.sub(pattern, repl, data)\\n    \\n    #Removing unknown words in data\\n    #for pattern in set(re.findall(\"<unk>\",data)):\\n    #    data=re.sub(pattern,repl,data)\\n    \\n    #Removing all the non-alphanumerical characters\\n    for pattern in set(re.findall(r\"[^\\\\w ]\", data)):\\n        repl=\\'\\'\\n        if pattern==\\'-\\':\\n            repl=\\' \\'\\n        #Retaining period, apostrophe\\n        if pattern!=\\'.\\' and pattern!=\"\\'\":\\n            data=re.sub(\"\\\\\"+pattern, repl, data)\\n            \\n    return data\\n\\nwikitext2_train_text = Clean_data(wikitext2_train_text)\\nwikitext2_val_text   = Clean_data(wikitext2_val_text)\\nwikitext2_test_text  = Clean_data(wikitext2_test_text)\\n\\nwith open(\"/kaggle/working/big_dos_train.txt\", \"w\") as file:\\n    file.write(x + wikitext2_train_text)\\n    \\nwith open(\"/kaggle/working/big_dos_val.txt\", \"w\") as file:\\n    file.write(wikitext2_val_text)\\n    \\nwith open(\"/kaggle/working/big_dos_test.txt\", \"w\") as file:\\n    file.write(wikitext2_test_text)\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Ejercicio 2 de la tarea 4 NLP**\nEste c√≥digo se basa en las notas de Norving:\nhttps://norvig.com/spell-correct.html\n","metadata":{}},{"cell_type":"code","source":"import re\nfrom collections import Counter\nimport itertools\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\nWORDS = Counter(words(open(\"/kaggle/input/big-dos-dataset/big_dos_train.txt\").read()))\n\ndef P(word, N=sum(WORDS.values())): \n    \"Probability of `word`.\"\n    return WORDS[word] / N\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word))  or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    letters = [let for let in letters]\n    \n    num_replaces = len(re.findall('_',word))\n    split_word = re.split('_',word) \n    repitted_letters = [letters for i in range(num_replaces)]\n    possible_comb = list(itertools.product(*repitted_letters))   \n    \n    \n    sub_candidatos = []\n    for i in range(len(possible_comb)):\n        new_word = split_word.copy()\n        for j in range(len(possible_comb[i])):\n            new_word.insert(2*j+1,possible_comb[i][j])\n        sub_candidatos.append(''.join(new_word))\n    return set(sub_candidatos)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:14:45.552261Z","iopub.execute_input":"2024-07-12T00:14:45.552873Z","iopub.status.idle":"2024-07-12T00:14:47.821472Z","shell.execute_reply.started":"2024-07-12T00:14:45.552830Z","shell.execute_reply":"2024-07-12T00:14:47.819787Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(correction('pe_p_e'),correction(\"phi__sop_y\"),correction(\"si_nif_c_nc_\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:14:47.823992Z","iopub.execute_input":"2024-07-12T00:14:47.824350Z","iopub.status.idle":"2024-07-12T00:14:48.843935Z","shell.execute_reply.started":"2024-07-12T00:14:47.824319Z","shell.execute_reply":"2024-07-12T00:14:48.842363Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"people philosophy significance\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Ejercicio 3**","metadata":{}},{"cell_type":"markdown","source":"# **3.1 Corpus y Modelo Estad√≠stico**\n\nLos datos de entrenamiento est√°n guardados en una variable llamada corpus.\nEstos datos no se enuentran procesados completamente.\n\nSe crea la clase \"ngram_model\" como respuesta a los incios 3.1.1 y 3.1.2\nLamentablemente, se tiene que crear otra clase m√°s adelante, pues se pide implementar un m√©todo smooth diferente para el ejercicio 3.1.3. Afortunadamente, esta nueva clase nos servir√° para lo que queda de la tarea.\n\n\n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade nltk\n# version de nltk mayor a 3.6 para poder usar concordance \nimport nltk\nnltk.__version__\n# si no funciona, click on \"Run\" -> \"Restart & clear cell outputs\"","metadata":{"execution":{"iopub.status.busy":"2024-07-18T21:47:13.650647Z","iopub.execute_input":"2024-07-18T21:47:13.651088Z","iopub.status.idle":"2024-07-18T21:47:33.858542Z","shell.execute_reply.started":"2024-07-18T21:47:13.651052Z","shell.execute_reply":"2024-07-18T21:47:33.857336Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'3.8.1'"},"metadata":{}}]},{"cell_type":"markdown","source":"Definimos una clase que hereda de Text (nltk) para los corpus y una funci√≥n que permite definir el vocabulario a partir de un umbral.","metadata":{}},{"cell_type":"markdown","source":"# **3.1.1 Cropus**","metadata":{}},{"cell_type":"markdown","source":"La idea original era usar esta clase ngram_corpus que heredad de Text para buscar los contexos en los que aparecen los ngramas. Sin embargo, aunque usabamos las fuinciones de b√∫squeda de Text, la b√∫squeda era tardada cuando los ngramas ten√≠an muchas coincidencias. Esto mejor√≥ un poco al incluir un diccionario de frecuencias para los unigramas. Sin embargo, sergu√≠a siendo lento si se quiere evaluar la perplejidad del conjunto de prueba (7 minutos en evaluar la perplejidad de un texto con 100 tokens).\nLa alternativa que se tom√≥ fue heredar de la clase MLE en nuestra clase modelo ya que tiene optimizadas las b√∫squedas de ngramas (probablemente con algo tipo un diccionario de frecuencias).\n\nNota: no era tan lento, solo hab√≠a que optimizar el uso de los unigramas","metadata":{}},{"cell_type":"code","source":"\"\"\"\nfrom nltk.text import Text\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.probability import FreqDist\n\n\nclass ngram_corpus(Text):\n    def __init__(self,corpus,n,threshold = 1):\n        self.n = n\n        self.threshold = threshold\n        # se procesa el corpus, pero a√∫n falta incorporar el threshold\n        tokenized_complete_corpus = self.processing_corpus(corpus)\n        # este es el vocabulario antes de hacer limpieza\n        complete_vocabulary = set(tokenized_complete_corpus)\n        pre_ngram_corpus = Text(tokenized_complete_corpus)\n        frecuencias = {}\n        for word in complete_vocabulary:\n            frecuencias[word] = len(pre_ngram_corpus.concordance_list(word,lines = 100**10000))\n        # el vocabulario se restringe a las palabras de mayor freuencia \n        # y tokens especiales\n        self.vocabulary = set([word for word in complete_vocabulary if frecuencias[word] >= self.threshold or word == \"<s>\" or word == \"</s>\" or word == \"<unk>\"]) | set([\"<unk>\"])   \n        # Los tokens del corpus que no formen parte de este vocabulario\n        # ser√°n sustituidas por \"<unk>\"\n        tokenized_corpus = [ ((word in self.vocabulary)*1)*word + (1-(word in self.vocabulary)*1)*\"<unk>\"  for word in tokenized_complete_corpus]\n        self.freqdist = FreqDist(tokenized_corpus)\n        c\n        \n    def processing_corpus(self,corpus,delete_nonalfa = False):\n        # Esta funci√≥n aplica preprocesamiento al corpus (string):\n        # 1) Si delete_nonalfa = True se queda con tokens o pal√°bras alfab√©ticos (tener en cuenta esto\n        # si por ejemplo, en lugar de eliminar el token put@ se desea sustituir por\n        # <unk>). delete_nonalfa = False sustituye los tokens no alfab√©ticos que sean n√∫meros por <unk>\n        # y los dem√°s los elimina.\n        # 2) Min√∫sculas: todo el texto en min√∫sculas\n        # 3) Agrega los tokens de inicio y final de oraci√≥n <s> y </s>.\n        # Regresa una lista tokenizada del corpus\n        n = self.n\n        processed_corpus = list()\n        for t in sent_tokenize(corpus):\n            if delete_nonalfa == False:\n                clear_sentence = [(token.isalpha()*1)*token.lower() + (1- token.isalpha()*1)*\"<unk>\" for token in word_tokenize(t) if token.isalnum()]\n            else:\n                clear_sentence = [token.lower()  for token in word_tokenize(t) if token.isalpha() == True]\n            processed_corpus.extend((n-1)*[\"<s>\"] + clear_sentence + (n-1)*[\"</s>\"])\n        return processed_corpus\n    \n    def count_senence(self,sentence):\n        # cuenta el n√∫mero de veces que aparece la oraci√≥n (en forma de lista)\n        # en el corpus. Revisar que la oraci√≥n haya sido traducida antes.\n        return len(self.concordance_list(sentence,lines = 10000000))\n    \n    def translate_new_corpus(self,new_corpus,delete_nonalfa = False):\n        # string a ser traducida\n        # esta frase est√° lista para ser usada por las funciones de probabilidad\n        \n        # se hace el preproceso que es el mismo que se hizo en el corpus\n        # de entrenamiento, pero en este caso, por defecto se cambian\n        # los tokens no alfab√©ticos por \"<unk>\"\n        tokenized_translated_corpus = self.processing_corpus(new_corpus,delete_nonalfa)\n        # regresamos la lista de tokens y los que no se encuentran en el \n        # vocabulario, se cambian por \"<unk>\"\n        return [ ((word in self.vocabulary)*1)*word + (1-(word in self.vocabulary)*1)*\"<unk>\"  for word in tokenized_translated_corpus]      \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-18T10:37:33.506010Z","iopub.execute_input":"2024-07-18T10:37:33.506391Z","iopub.status.idle":"2024-07-18T10:37:33.520231Z","shell.execute_reply.started":"2024-07-18T10:37:33.506364Z","shell.execute_reply":"2024-07-18T10:37:33.519131Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nprueba1 = ngram_corpus(\"HOla a Todos!, Soy Edgar. Respondi√≥ √©l. Edgar soy yo\",2)\nprint(\"Corpus procesado: \",\" \".join(prueba1))\n\nnueva_oracion = \"Hola Juan, soy Edgar\"\nprint(\"Original: \",nueva_oracion, \"Traducida: \", \" \".join(prueba1.translate_new_corpus(nueva_oracion)))\nprint(\"Original: \",nueva_oracion, \"Traducida: \", \" \".join(prueba1.translate_new_corpus(nueva_oracion,delete_nonalfa = True)))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:04:28.551256Z","iopub.execute_input":"2024-07-16T18:04:28.551711Z","iopub.status.idle":"2024-07-16T18:04:28.569580Z","shell.execute_reply.started":"2024-07-16T18:04:28.551677Z","shell.execute_reply":"2024-07-16T18:04:28.568162Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Corpus procesado:  <s> hola a todos soy edgar </s> <s> respondi√≥ √©l </s> <s> edgar soy yo </s>\nOriginal:  Hola Juan, soy Edgar Traducida:  <s> hola <unk> <unk> soy edgar </s>\nOriginal:  Hola Juan, soy Edgar Traducida:  <s> hola <unk> soy edgar </s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Para este primer ejercicio se decidi√≥ usar la clase Text de nltk para poder hacer la b√∫squeda de los ngramas, pues los textos que son transformados a este tipo de variables ya incluyen un m√©todo de b√∫squeda.\n\nM√°s adelante se usan listas anidadas para representar el corpus y hacer las b√∫squedas, eso con el objetivo de deshacernos de los cract√©res \"<s>\" y \"</s>\",  el objetivo es mejorar la predicci√≥n del modelo. ","metadata":{}},{"cell_type":"markdown","source":"# **3.1.2 Primer Modelo (Laplace smoothing)**","metadata":{}},{"cell_type":"code","source":"from nltk.text import Text\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom math import log,exp\n\n\nclass ngram_model:\n    def __init__(self,corpus,n,threshold = 1):\n        self.ngram = ngram_corpus(corpus,n,threshold)\n        #self.corpus_list_words_only = [term for term in self.corpus_list if term != \"<s>\" and term != \"</s>\"]    \n        \n        \n    def ngram_condit_log_prob(self,wk):\n        #wk = ['w1',..., \"wk\"], esta ya se enucentra traducida\n        #p[ w_n|w_(n-1), ..., w_(1)] = C(w_(1),..,w_(n))/C(w_(1),...,w_(n-1))\n        # si la cadena wn es de mayor longitud que n, enotnces aplicamos\n        # propiedad de Markov y nos quedamos solo con los n t√©rminos\n        m = min(self.ngram.n,len(wk))\n        if m >1:\n            log_prob = log(self.ngram.count_senence(wk) +1) - log(self.ngram.count_senence(wk[:(m-1)]) + len(self.ngram.vocabulary) )\n            #log_prob = log(self.ngram.count_senence(wk) ) - log(self.ngram.count_senence(wk[:(m-1)]) )\n        else:\n            log_prob = log(self.ngram.count_senence(wk[len(wk)-1]) +1) - log(len(self.ngram) +len(self.ngram.vocabulary))\n            #log_prob = log(self.ngram.count_senence(wk[len(wk)-1]) ) - log(len(self.ngram) )\n            \n        return log_prob\n    \n    def ngram_join_log_prob(self,wk):\n        # traducir la frase !!\n        #new_wk = self.ngram.translate_new_corpus(\" \".join(wk))\n        new_wk =wk\n        log_condit_probs = []\n        if new_wk[0] == \"<s>\":    \n            idx_first_let = len(new_wk)-new_wk[::-1].index(\"<s>\")-1\n            # no es el indice de la primera letra, pero es el correcto para\n            # calcular la probabilidad en este caso\n            idx_first_let += 1\n        else:\n            idx_first_let = 0 \n        for i in range(len(new_wk),idx_first_let,-1):\n            #print(wk[:i])\n            log_condit_probs.append(self.ngram_condit_log_prob(new_wk[:i]))\n        return sum(log_condit_probs)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T18:06:05.953885Z","iopub.execute_input":"2024-07-17T18:06:05.954326Z","iopub.status.idle":"2024-07-17T18:06:05.968375Z","shell.execute_reply.started":"2024-07-17T18:06:05.954292Z","shell.execute_reply":"2024-07-17T18:06:05.967233Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# La clase que creamos funciona bien con el ejemplo del libro\n# tener en cuenta que en la clase \"ngram_model\" se implementa smooth Laplace\n# si se quiere replicar el ejemplo del libro, modificar ngram_condit_log_prob\n# y la traducci√≥n autom√°tica que se hace en ngram_join_log_prob, ya que \n# esta agrega <s> y </s> al final e inicio de las oraciones\n\"\"\"\ncorpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\nmodelo_prueba = ngram_model(corpus,2)\nprint(exp( modelo_prueba.ngram_condit_log_prob( [\"<s>\",\"I\"] ) ))\nprint(modelo_prueba.ngram[:30])\n\nprint(exp( modelo_prueba.ngram_condit_log_prob( [\"am\"] ) ))\nprint(exp( modelo_prueba.ngram_condit_log_prob( [\"sam\"]) ))\nprint(exp( modelo_prueba.ngram_condit_log_prob( [\"am\",\"sam\"] ) ))\nprint(exp( modelo_prueba.ngram_condit_log_prob( [\"am\",\"</s>\"] ) ))\nprint(exp( modelo_prueba.ngram_join_log_prob( [\"am\",\"sam\"] ) ))\nprint(exp( modelo_prueba.ngram_join_log_prob( [\"i\"] ) ))\nprint(modelo_prueba.ngram.vocabulary)\n\n# este ejemplo no funciona cuando no se hace el smoothing Laplace\n# pues hay conteos que son ceros y por lo tanto, su log no est√° definido\n# √∫nicamente es para revisar que la prob condiconal sea efectivamente una\n# probabilidad. Una cuesti√≥n que habr√≠a que explorar es el hecho de\n# dar probabilidad igual a cero a cosas como P[<s>|hola] si es que se \n# desea generar solamente oraciones y no p√°rrafos completos. \np = 0\nfor term in modelo_prueba.ngram.vocabulary:\n    p += exp( modelo_prueba.ngram_condit_log_prob( [\"am\",term] ) )\nprint(p)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-16T17:14:18.150859Z","iopub.execute_input":"2024-07-16T17:14:18.151306Z","iopub.status.idle":"2024-07-16T17:14:18.174220Z","shell.execute_reply.started":"2024-07-16T17:14:18.151267Z","shell.execute_reply":"2024-07-16T17:14:18.172899Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"0.2\n['<s>', 'i', 'am', 'sam', '</s>', '<s>', 'sam', 'i', 'am', '</s>', '<s>', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']\n0.09374999999999999\n0.09374999999999999\n0.1428571428571429\n0.1428571428571429\n0.01339285714285715\n0.12499999999999997\n{'green', '</s>', 'ham', '<s>', 'sam', 'and', 'like', 'do', 'i', 'am', 'not', 'eggs'}\n1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A continuaci√≥n, se crea la clase \"ngram_model_interpol\" que ser√° la usada para el resto de los ejercicios de la tarea.\nEsta implementa el m√©todo de smoothing llamado interpolaci√≥n.\n\nAdem√°s de esto, se puede elgir el incluir o no los caract√©res 's'  \"/s\" para el inicio y final de oraciones. Gracias a las listas anidadas, no necestiamos de estos caracteres para distinguir entre oraciones, estas simplemente estar√°n separadas en listas diferentes.\n\nEl problema con no usar estos caracteres especiales es el hecho de que no se pueden hacer preguntas del estio \"¬øC√∫al es la probailidad de que una oraci√≥n empiece con la palabra \"you\"?, que en un modelo 2-grama se traducir√≠a a \nP[you|\"s\"]","metadata":{}},{"cell_type":"markdown","source":"# **3.1.3 Segundo Modelo (Interpolation smoothing)**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nfrom nltk.text import Text\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom math import log,exp\nimport numpy as np\n\nclass ngram_model_interpol:\n    def __init__(self,corpus,n,lambdas,threshold = 1):\n        # lambdas = [lambda_1,..,lambda_n] de modo que lambda_1 -> p(w_1)\n        # lambda_2 -> p(wk|w_(k-1)) , ...\n        self.ngram = ngram_corpus(corpus,n,threshold)\n        self.lambdas = lambdas\n        \n    def ngram_condit_log_interplol_prob(self,wk,lambdas = False):\n        # [ P(wk|w_(k-1),...w_(1)) ,P(wk|w_(k-1),...w_(2)) , P(w_k)]\n        m = min(self.ngram.n,len(wk))\n        for i in range(len(wk)-m,len(wk)):\n            inicio = time.time()\n            self.ngram_condit_log_prob(wk[i:])\n            final = time.time()\n            print(\"wk: \", wk[i:], \" tiempo: \", final-inicio)\n        log_cond_probs = [self.ngram_condit_log_prob(wk[i:]) for i in range(len(wk)-m,len(wk)) ]\n        #log_cond_probs.reverse()\n        if len(wk) < self.ngram.n:\n            extensor = (self.ngram.n-len(log_cond_probs))*[log_cond_probs[len(log_cond_probs)-1]]\n            log_cond_probs.extend(extensor)\n        if lambdas == False:\n            lambdas = self.lambdas[0]\n            return log_cond_probs\n            #return [log(sum([exp(log_cond_probs[i])*lambdas[i] for i in range(len(lambdas))]))]\n        return np.log(np.array(lambdas)@np.exp(np.array(log_cond_probs)))\n        #return [log(sum([exp(log_cond_probs[i])*used_lambdas[i] for i in range(len(used_lambdas))]))]\n        \n    def ngram_condit_log_prob(self,wk):\n        #wk = ['w1',..., \"wk\"], esta ya se enucentra traducida\n        #p[ w_n|w_(n-1), ..., w_(1)] = C(w_(1),..,w_(n))/C(w_(1),...,w_(n-1))\n        # si la cadena wn es de mayor longitud que n, enotnces aplicamos\n        # propiedad de Markov y nos quedamos solo con los n t√©rminos\n        m = min(self.ngram.n,len(wk))\n        if m >1:\n            if self.ngram.count_senence(wk[:(m-1)]) == 0 or self.ngram.count_senence(wk) == 0:\n            #if len(self.corpus_text.concordance_list(wk[:(m-1)],lines = 10*1000)) == 0 or len(self.corpus_text.concordance_list(wk,lines = 10*1000)) == 0:\n                log_prob = -float('inf')\n            else:\n                #log_prob = log(len(self.corpus_text.concordance_list(wk,lines = 10*1000))) - log(len(self.corpus_text.concordance_list(wk[:(m-1)],lines = 10*1000)) )\n                log_prob = log(self.ngram.count_senence(wk)) - log(self.ngram.count_senence(wk[:(m-1)])) \n        else:\n            #log_prob = log(self.ngram.count_senence(wk[len(wk)-1])) - log(len(self.ngram))\n            log_prob = log(self.ngram.freqdist[wk[len(wk)-1]]) - log(len(self.ngram))\n        return log_prob\n    \n    def ngram_join_log_prob(self,wk,lambdas = False):\n    # calcula la probabilidad conjunta del vector wk.\n    #  lambdas deber√° ser una lista de listas de pesos\n    # y se regresar√° la probabilidad conjunta de wk para todas las lambdas.\n        # traducir la frase !!\n        #new_wk = self.ngram.translate_new_corpus(\" \".join(wk))\n        new_wk = wk\n        \n        #if new_wk[0] == \"<s>\":    \n        #    idx_first_let = len(new_wk)-new_wk[::-1].index(\"<s>\")-1\n            # no es el indice de la primera letra, pero es el correcto para\n            # calcular la probabilidad en este caso\n        #    idx_first_let += 1\n        #else:\n        #    idx_first_let = 0 \n        idx_first_let = 0\n        if lambdas == False:\n            lambdas = self.lambdas\n        log_condit_probs = np.zeros((len(lambdas),len(new_wk) -idx_first_let))\n        #print(log_condit_probs.shape)\n        j=0\n        for i in range(len(new_wk),idx_first_let,-1):\n            #print(wk[:i])\n            log_condit_probs[:,j] = self.ngram_condit_log_interplol_prob(new_wk[:i],lambdas)\n            #log_condit_probs[:,j] = np.matrix([[1.,1.]])\n            #print(log_condit_probs)\n            j+=1\n        #print(log_condit_probs.shape)\n        return np.sum(log_condit_probs,axis = 1)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-18T11:14:36.912355Z","iopub.execute_input":"2024-07-18T11:14:36.912780Z","iopub.status.idle":"2024-07-18T11:14:36.929360Z","shell.execute_reply.started":"2024-07-18T11:14:36.912748Z","shell.execute_reply":"2024-07-18T11:14:36.928032Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Porbando el modelo calculando las probabilidades de los mismos\n# eventos. Puede que ya no den las cuentas.\ndef clear_sentence(sentence_list):\n    return [token for token in sentence_list if token.isalpha() == True]\n\ncorpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\nmodelo_prueba = ngram_model_interpol(corpus,2,[[.9,.1]])\nprint(exp( modelo_prueba.ngram_condit_log_prob( [\"<s>\",\"i\"] ) ))\nprint(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"<s>\",\"i\"] ) ))\nprint(modelo_prueba.ngram[:30])\n\nprint(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\"] ) ))\nprint(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"sam\"]) ))\nprint(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\",\"sam\"] ) ))\nprint(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\",\"</s>\"] ) ))\nprint(exp( modelo_prueba.ngram_join_log_prob( [\"am\",\"sam\"] ) ))\nprint(exp( modelo_prueba.ngram_join_log_prob( [\"i\"] ) ))\nprint(modelo_prueba.ngram.vocabulary)\n\np = 0\nfor term in modelo_prueba.ngram.vocabulary:\n    if term != \"<unk>\":\n        p += exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\",term] ) )\nprint(p)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-17T20:05:42.475582Z","iopub.execute_input":"2024-07-17T20:05:42.476102Z","iopub.status.idle":"2024-07-17T20:05:42.506731Z","shell.execute_reply.started":"2024-07-17T20:05:42.476062Z","shell.execute_reply":"2024-07-17T20:05:42.505302Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"0.6666666666666666\n0.20166666666666672\n['<s>', 'i', 'am', 'sam', '</s>', '<s>', 'sam', 'i', 'am', '</s>', '<s>', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']\n0.10000000000000002\n0.10000000000000002\n0.14\n0.18500000000000005\n0.014000000000000007\n0.15000000000000002\n{'<unk>', 'like', 'sam', 'green', 'i', 'am', '</s>', 'do', 'not', 'and', 'eggs', '<s>', 'ham'}\n1.0000000000000002\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/3448793311.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"<s>\",\"i\"] ) ))\n/tmp/ipykernel_34/3448793311.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\"] ) ))\n/tmp/ipykernel_34/3448793311.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"sam\"]) ))\n/tmp/ipykernel_34/3448793311.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\",\"sam\"] ) ))\n/tmp/ipykernel_34/3448793311.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\",\"</s>\"] ) ))\n/tmp/ipykernel_34/3448793311.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(exp( modelo_prueba.ngram_join_log_prob( [\"am\",\"sam\"] ) ))\n/tmp/ipykernel_34/3448793311.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(exp( modelo_prueba.ngram_join_log_prob( [\"i\"] ) ))\n/tmp/ipykernel_34/3448793311.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  p += exp( modelo_prueba.ngram_condit_log_interplol_prob( [\"am\",term] ) )\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.text import Text\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom math import log,exp\nimport numpy as np\nfrom nltk.lm import MLE\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nfrom nltk.probability import FreqDist\n\ndef clear_sentence(sentence):\n    \"\"\"\n    sentence: oraci√≥n en forma de string como la que devuelve sent_tokenize\n    regresa : lista de tokens limpia. Quitando todo lo que no sen palabras o n√∫meros\n              En caso de ser n√∫meros, estos se cambian por \"<UNK>\"\n    \"\"\"\n    return [((w.isalpha())*1)*w.lower() + (1-(w.isalpha())*1)*\"<UNK>\"  for w in word_tokenize(sentence) if w.isnumeric() or w.isalpha()] \n\nclass ngram_model_interpol(MLE):\n    def __init__(self,corpus,n,lambdas,threshold = 1):\n        \"\"\"\n        # lambdas = [[lambda_1,..,lambda_n]] de modo que lambda_1 -> p(w_1)\n        # lambda_2 -> p(wk|w_(k-1)) , ...   OJO: lista de lista para lambdas\n        \"\"\"\n        # si se desea proceder de manera diferente con la limpieza del corpus, \n        # cambiar la funci√≥n clear_sentence\n        tokenized_corpus = [clear_sentence(t) for t in sent_tokenize(corpus)]\n        # vocabulario completo\n        complete_vocab = set([w for sentence in tokenized_corpus for w in sentence]) \n        # frecuencias de cada t√©rmino en el corpus procesado\n        F = FreqDist([w for sentence in tokenized_corpus for w in sentence])\n        # el vocabulario final. Como a√∫n no se agregan los tokens de inico y final\n        # de oraci√≥n en la tokenizaci√≥n, se agregan aparte en el vocabulario.\n        vocab_final = set([w for sentence in tokenized_corpus for w in sentence if F[w]>= threshold]) | set([\"<s>\"]) | set([\"</s>\"]) | set([\"<UNK>\"])\n        # Los objetos necesarios para hacer el ajuste del modelo\n        train, vocab = padded_everygram_pipeline(n, tokenized_corpus) \n        super().__init__(n)\n        self.fit(train,vocab_final)\n        self.n = n\n        self.lambdas = lambdas\n        self.vocabulario = vocab_final\n    \n    def log_join_prob(self,wk,lambdas = False):\n        \"\"\"\n        wk : una lista con los tokens de la oraci√≥n ya traducida al vocabulario del corpus\n        lambdas :lista de listas [lambda1_list,lambda2_list,...]\n        \"\"\"\n        if lambdas == False:\n            lambdas = self.lambdas\n        LJP = np.zeros((len(lambdas),len(wk)))\n        for i in range(len(wk)):\n            LJP[:,i] = self.log_interpol_prob(wk[:(i+1)],lambdas)\n        return np.sum(LJP,axis = 1)\n    \n    def log_interpol_prob(self,wk,lambdas = False):\n        \"\"\"\n        lambdas : lista de listas [lambda1_list,lambda2_list,...]\n        return  : regresa la probabilidad calculada con cada lambdai_list\n        \"\"\"\n        m = max(0,len(wk)-self.n)\n        LCP = [self.log_cond_prob(wk[i:]) for i in range(len(wk)-1,m-1,-1)]\n        if len(wk) < self.n:\n            LCP.extend((self.n-len(wk))*[LCP[len(LCP)-1]])\n        if lambdas == False:\n            lambdas = self.lambdas\n        return np.log2(np.array(lambdas)@np.exp2(np.array(LCP)))\n    \n    def log_cond_prob(self,wk):\n        \"\"\"\n        wk : una lista con los tokens de la oraci√≥n ya traducida al vocabulario del corpus\n        return: P[w_k | w_(k-1),...,w_1] bajo el modelo.\n        \"\"\"\n        m = max(0,len(wk)-self.n)\n        if len(wk) > 1:\n            return self.logscore(wk[len(wk)-1],wk[m:(len(wk)-1)])\n        else:\n            return self.logscore(wk[len(wk)-1])\n    def traducir(self,new_corpus):\n        corpus_tokenizado = []\n        n =self.n\n        for t in sent_tokenize(new_corpus):\n            corpus_tokenizado.extend((n-1)*[\"<s>\"] + clear_sentence(t) + (n-1)*[\"</s>\"])\n        \n        return [((word in self.vocabulario)*1)*word + (1-(word in self.vocabulario)*1)*\"<UNK>\"  for word in corpus_tokenizado]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T22:07:57.190019Z","iopub.execute_input":"2024-07-18T22:07:57.190552Z","iopub.status.idle":"2024-07-18T22:07:57.217075Z","shell.execute_reply.started":"2024-07-18T22:07:57.190481Z","shell.execute_reply":"2024-07-18T22:07:57.215810Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\ncorpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\nmodelo_prueba = ngram_model_interpol(corpus,2,[[.9,.1]])\nprint(2**modelo_prueba.log_cond_prob( [\"<s>\",\"i\"] ) )\nprint(2**modelo_prueba.log_interpol_prob( [\"<s>\",\"i\"] ) )\nprint(\"------------------------------------\")\n\nprint(2**modelo_prueba.log_interpol_prob( [\"am\"] ) )\nprint(2**modelo_prueba.log_interpol_prob( [\"sam\"]) )\nprint(2**modelo_prueba.log_interpol_prob( [\"am\",\"sam\"] ) )\nprint(2**modelo_prueba.log_interpol_prob( [\"am\",\"</s>\"] ) )\nprint(2**modelo_prueba.log_join_prob( [\"am\",\"sam\"] ) )\nprint(2**modelo_prueba.log_join_prob( [\"i\"] ) )\nprint(modelo_prueba.vocabulario)\n\np = 0\nfor term in modelo_prueba.vocabulario:\n    p += 2**modelo_prueba.log_interpol_prob( [\"am\",term] ) \nprint(p)\n\nprint(modelo_prueba.traducir(\"Hi, I am Juan and I do not know sam. I Like 3 green eggs! \"))\nprint(modelo_prueba.traducir(\"I am Sam. Sam I am. I do not like green eggs and ham 4.\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T22:08:06.879141Z","iopub.execute_input":"2024-07-18T22:08:06.879595Z","iopub.status.idle":"2024-07-18T22:08:06.898424Z","shell.execute_reply.started":"2024-07-18T22:08:06.879563Z","shell.execute_reply":"2024-07-18T22:08:06.896865Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"0.6666666666666666\n[0.20166667]\n------------------------------------\n[0.1]\n[0.1]\n[0.14]\n[0.185]\n[0.014]\n[0.15]\n{'sam', 'green', 'eggs', 'am', '<UNK>', 'like', '</s>', 'not', 'i', 'do', 'and', 'ham', '<s>'}\n[1.]\n['<s>', '<UNK>', 'i', 'am', '<UNK>', 'and', 'i', 'do', 'not', '<UNK>', 'sam', '</s>', '<s>', 'i', 'like', '<UNK>', 'green', 'eggs', '</s>']\n['<s>', 'i', 'am', 'sam', '</s>', '<s>', 'sam', 'i', 'am', '</s>', '<s>', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '<UNK>', '</s>']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/1395396386.py:64: RuntimeWarning: divide by zero encountered in log2\n  return np.log2(np.array(lambdas)@np.exp2(np.array(LCP)))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Leemos el corpus para los datos de entrenamiento, validaci√≥n y prueba.","metadata":{}},{"cell_type":"code","source":"from nltk.text import Text\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\ncorpus = open(\"/kaggle/input/big-dos-dataset/big_dos_train.txt\").read()\nval_corpus = open(\"/kaggle/input/big-dos-dataset/big_dos_val.txt\").read()\ntest_corpus = open(\"/kaggle/input/big-dos-dataset/big_dos_test.txt\").read()\n\nlambda_grid = [[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1] , [.1, .4, .5]]","metadata":{"execution":{"iopub.status.busy":"2024-07-18T22:35:09.078452Z","iopub.execute_input":"2024-07-18T22:35:09.078935Z","iopub.status.idle":"2024-07-18T22:35:09.276603Z","shell.execute_reply.started":"2024-07-18T22:35:09.078902Z","shell.execute_reply":"2024-07-18T22:35:09.275396Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Creamos el modelo y traducimos el corpus de validaci√≥n y prueba.\nLas lambdas proporcionadas para la instansiaci√≥n del modelo pueden ser\nmodificadas al moment de calcular la probailidad conjunta.","metadata":{}},{"cell_type":"code","source":"# modelo de tri gramas con threshold = 3.\nmodelo_ejercicio3 = ngram_model_interpol(corpus,3,[[1/3,1/3,1/3]],4)\ntranslated_val_corpus = modelo_ejercicio3.traducir(val_corpus)\ntranslated_test_corpus = modelo_ejercicio3.traducir(test_corpus)\nlen(modelo_ejercicio3.vocabulario)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T22:35:21.417404Z","iopub.execute_input":"2024-07-18T22:35:21.418889Z","iopub.status.idle":"2024-07-18T22:38:05.425996Z","shell.execute_reply.started":"2024-07-18T22:35:21.418819Z","shell.execute_reply":"2024-07-18T22:38:05.424863Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"28040"},"metadata":{}}]},{"cell_type":"markdown","source":"\nEligiendo el lambra basado en la perplejidad","metadata":{}},{"cell_type":"code","source":"small_val = translated_val_corpus\nlen(small_val)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T22:39:16.399067Z","iopub.execute_input":"2024-07-18T22:39:16.400382Z","iopub.status.idle":"2024-07-18T22:39:16.410174Z","shell.execute_reply.started":"2024-07-18T22:39:16.400334Z","shell.execute_reply":"2024-07-18T22:39:16.406455Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"211983"},"metadata":{}}]},{"cell_type":"code","source":"import time\nlog_perplexity = []\nN_val = len([token for token in small_val if token != \"<s>\"])\ninicio = time.time()\nlog_perplexity = modelo_ejercicio3.log_join_prob(small_val,lambda_grid)\nfinal = time.time()\nprint(final-inicio,\"\\n\")\nfor i in range(len(lambda_grid)):\n    #print(\"Lambda: \",lambda_grid[i], \", Perplexity: \", exp(-(1/N_val)*log_perplexity[i]))\n    print(\"Lambda: \",lambda_grid[i], \", Perplexity: \", 2**(-(1/N_val)*log_perplexity[i]) )","metadata":{"execution":{"iopub.status.busy":"2024-07-18T22:39:23.013040Z","iopub.execute_input":"2024-07-18T22:39:23.013478Z","iopub.status.idle":"2024-07-18T22:43:58.397734Z","shell.execute_reply.started":"2024-07-18T22:39:23.013445Z","shell.execute_reply":"2024-07-18T22:43:58.396340Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"275.3462851047516 \n\nLambda:  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333] , Perplexity:  370.73153230672983\nLambda:  [0.4, 0.4, 0.2] , Perplexity:  355.44708241570027\nLambda:  [0.2, 0.4, 0.4] , Perplexity:  391.3672732214248\nLambda:  [0.5, 0.4, 0.1] , Perplexity:  361.41319167657315\nLambda:  [0.1, 0.4, 0.5] , Perplexity:  456.99800594729453\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **3.2 Corrector ortogr√°fico** ","metadata":{}},{"cell_type":"markdown","source":"Para corregir una frase dada, se crean primero candidatos usando el modelo de Norvig (Ejercicio 2). Despu√©s, se elige el mejor de entre los candidatos, maximizando una probabilidad.\n\nA continuaci√≥n se cargan las funciones de Norving para generar candidatos.","metadata":{}},{"cell_type":"code","source":"import re\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text)\n\n#WORDS = Counter(words(open('/kaggle/input/big-dos-dataset/big_dos_train.txt').read()))\nWORDS = modelo_ejercicio3_2.vocabulario\n\ndef candidates(word): \n    \"\"\"\"\n    Generate possible spelling corrections for word.\n    Dos maneras de generarlos: si la palabra es conocida, no hacer cambios\n    de cualquier otro modo, buscar candidatos con edits. La segunda es\n    generar candidatos con edits sin importar si la palabra es conocida\n    \"\"\"\n    if word in WORDS:\n        return set([word])\n    else:\n        return  known(edits1(word)) | known(edits2(word))  \n    #return known([word]) | known(edits1(word)) | known(edits2(word)) \n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T00:32:27.311060Z","iopub.execute_input":"2024-07-19T00:32:27.311570Z","iopub.status.idle":"2024-07-19T00:32:27.327245Z","shell.execute_reply.started":"2024-07-19T00:32:27.311536Z","shell.execute_reply":"2024-07-19T00:32:27.325788Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"Existen dos estrategias inmediatas para seleccionar la oraci√≥n final corregida:\n1) Seleccionar los candidatos de manera secuencial: Esta se basa en maximizar la probabilidad condicional del nuevo candidato dado que ya se tiene la oraci√≥n corregida hasta la palabra anterior\n\n2) De entre todas las posibles oraciones formadas por los diferentes candidatos, elegir la oraci√≥n que tenga m√°xima probabilidad conjunta\n\nPrimero ajustamos el modelo con los valores de lambda que tuvieron menor perplejidad.","metadata":{}},{"cell_type":"code","source":"corpus = open(\"/kaggle/input/big-dos-dataset/big_dos_train.txt\").read()\nmodelo_ejercicio3_2 = ngram_model_interpol(corpus,3,[[.4,.4,.2]],2)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T00:06:56.925392Z","iopub.execute_input":"2024-07-19T00:06:56.926836Z","iopub.status.idle":"2024-07-19T00:09:35.291442Z","shell.execute_reply.started":"2024-07-19T00:06:56.926789Z","shell.execute_reply":"2024-07-19T00:09:35.290309Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def correcion_ortografica(wk):\n    candidatos_dict = {}\n    for i in range(len(wk)):\n        candidatos_dict[i] = set([ ((c in modelo_ejercicio3_2.vocabulario)*1)*c + (1-(c in modelo_ejercicio3_2.vocabulario)*1)*\"<UNK>\" for c in candidates(wk[i])])\n    \n    #print(candidatos_dict)\n    #sequential_sentence = [\"<s>\"]\n    sequential_sentence = []\n    for i in range(len(wk)):\n        sequential_sentence = max([sequential_sentence + [c] for c in candidatos_dict[i] ],key = modelo_ejercicio3_2.log_interpol_prob) \n    \n    #power_set_candidates = [[\"<s>\"]]\n    power_set_candidates = [[]]\n    for i in range(len(wk)):\n        power_set_candidates = [ s  + [c] for c in  candidatos_dict[i] for s in power_set_candidates]\n    #print(power_set_candidates)\n    non_sequential_sentence = max(power_set_candidates, key = modelo_ejercicio3_2.log_join_prob)\n    return (sequential_sentence,non_sequential_sentence)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-19T00:16:18.754996Z","iopub.execute_input":"2024-07-19T00:16:18.755810Z","iopub.status.idle":"2024-07-19T00:16:18.766766Z","shell.execute_reply.started":"2024-07-19T00:16:18.755768Z","shell.execute_reply":"2024-07-19T00:16:18.765518Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"frase1 = \"you heva\"\nprint(correcion_ortografica(frase1.split()))\nprint(correcion_ortografica([ \"i\"  ,  \"hav\" ,  \"a\"  , \"ham\"  ]))\nprint(correcion_ortografica([ \"my\"  ,  \"countr\"  ,  \"is\"  ,  \"biig\"  ]))\nprint(correcion_ortografica([  \"the\" ,  \"science\"  ,  \"0ff\"  ,  \"computer\"  ]))\nprint(correcion_ortografica([  \"i\"  ,  \"want\"  ,  \"t00\"  ,  \"eat\"  ]))\n\n\nprint(modelo_ejercicio3_2.log_join_prob([  \"the\"  ,  \"sciens\"  ,  \"off\"  ,  \"math\"  ]))\nprint(modelo_ejercicio3_2.log_join_prob([  \"the\"  ,  \"sciens\"  ,  \"of\"  ,  \"math\"  ]))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T00:34:50.871131Z","iopub.execute_input":"2024-07-19T00:34:50.871536Z","iopub.status.idle":"2024-07-19T00:34:51.557328Z","shell.execute_reply.started":"2024-07-19T00:34:50.871504Z","shell.execute_reply":"2024-07-19T00:34:51.555920Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"(['you', 'have'], ['you', 'have'])\n(['i', 'hav', 'a', 'ham'], ['i', 'hav', 'a', 'ham'])\n(['my', 'country', 'is', 'being'], ['my', 'country', 'is', 'being'])\n(['the', 'science', 'of', 'computer'], ['the', 'science', 'of', 'computer'])\n(['i', 'want', 'to', 'eat'], ['i', 'want', 'to', 'eat'])\n(['the', 'scene', 'off', 'math'], ['the', 'scene', 'off', 'math'])\n[-45.68528094]\n[-38.01182882]\n","output_type":"stream"}]},{"cell_type":"code","source":"def autocompletar(wk):\n    conditional_candidate = max([wk + [c] for c in modelo_ejercicio3_2.vocabulario],key = modelo_ejercicio3_2.log_interpol_prob)\n    join_candidate = max([wk + [c] for c in modelo_ejercicio3_2.vocabulario],key = modelo_ejercicio3_2.log_join_prob)\n    return (conditional_candidate,join_candidate)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T00:44:24.658476Z","iopub.execute_input":"2024-07-19T00:44:24.658990Z","iopub.status.idle":"2024-07-19T00:44:24.667259Z","shell.execute_reply.started":"2024-07-19T00:44:24.658951Z","shell.execute_reply":"2024-07-19T00:44:24.665670Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"autocompletar(\"i would like to visit new\".split())","metadata":{"execution":{"iopub.status.busy":"2024-07-19T00:45:39.179824Z","iopub.execute_input":"2024-07-19T00:45:39.180307Z","iopub.status.idle":"2024-07-19T00:45:55.132261Z","shell.execute_reply.started":"2024-07-19T00:45:39.180274Z","shell.execute_reply":"2024-07-19T00:45:55.131163Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"(['i', 'would', 'like', 'to', 'visit', 'new', 'york'],\n ['i', 'would', 'like', 'to', 'visit', 'new', 'york'])"},"metadata":{}}]}]}
